{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Sequence-to-Sequence: Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment you will use a database of pairs of (English,French) sentences to train an RNN model to translate from English to French.\n",
    "\n",
    "The directory ../resource/asnlib/publicdata contains two files, \"small_vocab_en.txt\" and \"small_vocab_fr.txt\". Line \"n\" of the first file corresponds to line \"n\" of the second file.\n",
    "\n",
    "Also see data here: http://www.statmt.org/wmt14/translation-task.html\n",
    "\n",
    "Keras resources: \n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "* https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563\n",
    "\n",
    "Neural Language Translation Resources:\n",
    "* https://arxiv.org/abs/1703.01619\n",
    "* https://www.tensorflow.org/tutorials/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, Embedding, Flatten, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tensorflow to be less aggressive about RAM utilization when it starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1  # Start with 10% of the GPU RAM\n",
    "config.gpu_options.allow_growth = True                    # Dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)                                         # Set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7           # % of data in training set\n",
    "\n",
    "NUM_LSTM_NODES = 256             # Num of intermediate LSTM nodes\n",
    "CONTEXT_VECTOR_SIZE = 256        # Size of context vector (num of LSTM nodes in final LSTM layer)\n",
    "\n",
    "EMBEDDING_DIM = 100              # Embedding layer size for input words\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "NUM_DATA_EXAMPLES = 5000         # limit memory usage\n",
    "\n",
    "LR = 0.01\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_space_around_punctuation(s):\n",
    "    result = ''\n",
    "    for c in s:\n",
    "        if c in string.punctuation and c != \"'\":\n",
    "            result += ' %s ' % c\n",
    "        else:\n",
    "            result += c\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PUNCTUATION_REMOVER = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "def clean_sentence(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    #s = s.translate(PUNCTUATION_REMOVER)\n",
    "    s = add_space_around_punctuation(s)\n",
    "    return s\n",
    "\n",
    "def get_words_from_sentence(s, add_start_symbol=False, add_end_symbol=True, reverse=False):\n",
    "#def get_words_from_sentence(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    words = list(filter(None, s.split(' ')))\n",
    "    if reverse:\n",
    "        words = words[::-1]\n",
    "    if add_start_symbol:\n",
    "        words = ['<S>'] + words\n",
    "    if add_end_symbol:\n",
    "        words.append('</S>')\n",
    "    return words\n",
    "\n",
    "def get_word_list_from_sentence_string(s, add_start_symbol=False, add_end_symbol=True, reverse=False):\n",
    "    return get_words_from_sentence(clean_sentence(s), add_start_symbol, add_end_symbol, reverse)    \n",
    "    \n",
    "def get_sentences(path, filename, add_start_symbol=False, add_end_symbol=True, reverse=False):\n",
    "#def get_sentences(path, filename, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [get_word_list_from_sentence_string(s, add_start_symbol, add_end_symbol, reverse) \n",
    "                for s in lines]\n",
    "\n",
    "def get_word_set(sentences):\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words.add(word)\n",
    "    return words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH = '../resource/asnlib/publicdata'\n",
    "s1 = get_sentences(PATH, 'small_vocab_en.txt', reverse=True, add_end_symbol=False) # Reverse input sentences for training\n",
    "s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True)\n",
    "#s1 = get_sentences(PATH, 'small_vocab_en.txt', reverse=False, add_end_symbol=False) # Reverse input sentences for training\n",
    "#s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Only get a subset of the data\n",
    "s1 = s1[:NUM_DATA_EXAMPLES]\n",
    "s2 = s2[:NUM_DATA_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1 = list(get_word_set(s1))\n",
    "w2 = list(get_word_set(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dislike\n",
      "nice\n",
      "elephants\n",
      "didn't\n",
      "did\n",
      "they\n",
      "wonderful\n",
      "translate\n",
      "weather\n",
      "easy\n",
      "\n",
      "ne\n",
      "enneigée\n",
      "aimée\n",
      "dernière\n",
      "inde\n",
      "préféré\n",
      "l'oiseau\n",
      "occupé\n",
      "allez\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "for w in w1[:10]:\n",
    "    print(w)\n",
    "print()\n",
    "for w in w2[:10]:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_to_index_dict(words):\n",
    "    return {w: i+1 for i,w in enumerate(words)}  # use i+1 to reserve 0 for the mask index\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_to_index1 = get_word_to_index_dict(w1)\n",
    "word_to_index2 = get_word_to_index_dict(w2)\n",
    "index_to_word1 = reverse_dict(word_to_index1)\n",
    "index_to_word2 = reverse_dict(word_to_index2)\n",
    "index_to_word1[0] = '<MASK>'\n",
    "index_to_word2[0] = '<MASK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(s, word_to_index):\n",
    "    return [word_to_index[w] for w in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indices_to_sentence(indices, index_to_word):\n",
    "    return [index_to_word[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[171, 33, 46, 20, 304, 122]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sentence_to_indices(get_word_list_from_sentence_string('vous aimez raisins.', add_start_symbol=True), word_to_index2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S>', 'vous', 'aimez', 'raisins', '.', '</S>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_to_sentence(x, index_to_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 310)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_X = len(w1) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_y = len(w2) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_X, num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs_as_indices = [sentence_to_indices(s, word_to_index1) for s in s1]\n",
    "outputs_as_indices = [sentence_to_indices(s, word_to_index2) for s in s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = pad_sequences(inputs_as_indices)\n",
    "outputs = pad_sequences(outputs_as_indices, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 24)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len_X = len(inputs[0])\n",
    "max_seq_len_y = len(outputs[0])\n",
    "max_seq_len_X, max_seq_len_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, \n",
    "                                                    test_size=1 - TRAIN_TEST_SPLIT,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need to one-hot-encode the outputs ourselves for use in the loss function. \n",
    "# The inputs get this for free via use of Embedding layers in Keras.\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3499, 17), (1501, 17), (3499, 24), (1501, 24))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now we need to write code to build the SeqToSeq model. **Important**: In Keras we have to use the \"functional API\" in order to access the LSTM internal state that we use as the \"context vector\" or \"encoding\" of a sentence. We also need to store hooks into the model to be able to run the translator on new sentences after training.\n",
    "\n",
    "This code will create variables representing the entire SeqToSeq model (for use in training), as well as the individual encoder segment and decoder segment of the model, for use in inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the following architecture:\n",
    "    \n",
    "1. Encoder input: shape (max_seq_len_X,)\n",
    "2. Masking layer: doesn't change shape. Ignores leading \"-1\"s in shorter sequences.\n",
    "3. Embedding layer: output shape (max_seq_len_X, EMBEDDING_DIM)\n",
    "4. LSTM layer 1. N.B. The first LSTM layer in a stack must use \"return_sequences=True\"\n",
    "5. LSTM layer 2. N.B. The final layer must use \"return_state=True\" so we can extract the internal state (the context vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build RNN model.\n",
    "# See also: https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
    "\n",
    "encoding_size = CONTEXT_VECTOR_SIZE\n",
    "max_input_seq_len = max_seq_len_X\n",
    "max_output_seq_len = max_seq_len_y\n",
    "num_input_words = num_words_X\n",
    "num_output_words = num_words_y\n",
    "\n",
    "encoder_inputs = Input(shape=(max_input_seq_len,), name='encoder_input')\n",
    "encoder_inputs_masked = Masking(mask_value=0, name='encoder_masking')(encoder_inputs)\n",
    "encoder_inputs_embedded = Embedding(num_input_words, EMBEDDING_DIM, mask_zero=True, name='encoder_embedding')(encoder_inputs_masked)\n",
    "encoder_outputs1, state_h1, state_c1 = LSTM(NUM_LSTM_NODES, return_sequences=False, return_state=True,\n",
    "                                            name='encoder_lstm_1')(encoder_inputs_embedded)\n",
    "\n",
    "# Discard `encoder_outputs2` and only keep the states.\n",
    "encoder_states1 = [state_h1, state_c1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder should have the following architecture:\n",
    "    \n",
    "1. Decoder input: shape (max_seq_len_y,)\n",
    "2. Masking layer: doesn't change shape. Ignores final \"-1\"s in shorter sequences.\n",
    "3. Embedding layer: output shape (max_seq_len_y, EMBEDDING_DIM)\n",
    "4. LSTM layer 1. N.B. The first LSTM layer in a stack must use \"return_sequences=True\"\n",
    "5. LSTM layer 2. N.B. The final layer must use \"return_state=True\" so we can extract the internal state.\n",
    "6. Dense layer with softmax: output shape (num_output_words,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_inputs_masked = Masking(mask_value=0, name='decoder_masking')(decoder_inputs)\n",
    "decoder_inputs_embedded = Embedding(num_output_words, EMBEDDING_DIM, mask_zero=True, \n",
    "                                    name='decoder_embedding')(decoder_inputs_masked)\n",
    "decoder_lstm_1 = LSTM(NUM_LSTM_NODES, return_sequences=True, return_state=True, name='decoder_lstm_1')\n",
    "z, _, _ = decoder_lstm_1(decoder_inputs_embedded, initial_state=encoder_states1)\n",
    "decoder_dense = Dense(num_output_words, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(z)\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Put it all together into one model, and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_masking (Masking)       (None, 17)           0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_masking (Masking)       (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, 17, 100)      19700       encoder_masking[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 100)    31000       decoder_masking[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)           [(None, 256), (None, 365568      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 310)    79670       decoder_lstm_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,506\n",
      "Trainable params: 861,506\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"470pt\" viewBox=\"0.00 0.00 839.50 470.00\" width=\"840pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 835.5,-466 835.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140432637700976 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140432637700976</title>\n",
       "<polygon fill=\"none\" points=\"75,-415.5 75,-461.5 367,-461.5 367,-415.5 75,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-434.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-415.5 236,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-438.5 291,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-415.5 291,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-446.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"291,-438.5 367,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-423.3\">(None, 17)</text>\n",
       "</g>\n",
       "<!-- 140432637701592 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140432637701592</title>\n",
       "<polygon fill=\"none\" points=\"71.5,-332.5 71.5,-378.5 370.5,-378.5 370.5,-332.5 71.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-351.8\">encoder_masking: Masking</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-332.5 239.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-355.5 294.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-332.5 294.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.5\" y=\"-363.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-355.5 370.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.5\" y=\"-340.3\">(None, 17)</text>\n",
       "</g>\n",
       "<!-- 140432637700976&#45;&gt;140432637701592 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140432637700976-&gt;140432637701592</title>\n",
       "<path d=\"M221,-415.366C221,-407.152 221,-397.658 221,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-388.607 221,-378.607 217.5,-388.607 224.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432530516120 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140432530516120</title>\n",
       "<polygon fill=\"none\" points=\"492,-332.5 492,-378.5 800,-378.5 800,-332.5 492,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"572.5\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"653,-332.5 653,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"680.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"653,-355.5 708,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"680.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"708,-332.5 708,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"754\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"708,-355.5 800,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"754\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140432530515896 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140432530515896</title>\n",
       "<polygon fill=\"none\" points=\"488.5,-249.5 488.5,-295.5 803.5,-295.5 803.5,-249.5 488.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"572.5\" y=\"-268.8\">decoder_masking: Masking</text>\n",
       "<polyline fill=\"none\" points=\"656.5,-249.5 656.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"656.5,-272.5 711.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"711.5,-249.5 711.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"757.5\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"711.5,-272.5 803.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"757.5\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140432530516120&#45;&gt;140432530515896 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140432530516120-&gt;140432530515896</title>\n",
       "<path d=\"M646,-332.366C646,-324.152 646,-314.658 646,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"649.5,-305.607 646,-295.607 642.5,-305.607 649.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432637701816 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140432637701816</title>\n",
       "<polygon fill=\"none\" points=\"43,-249.5 43,-295.5 399,-295.5 399,-249.5 43,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-268.8\">encoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"240,-249.5 240,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"240,-272.5 295,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"295,-249.5 295,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"347\" y=\"-280.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"295,-272.5 399,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"347\" y=\"-257.3\">(None, 17, 100)</text>\n",
       "</g>\n",
       "<!-- 140432637701592&#45;&gt;140432637701816 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140432637701592-&gt;140432637701816</title>\n",
       "<path d=\"M221,-332.366C221,-324.152 221,-314.658 221,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-305.607 221,-295.607 217.5,-305.607 224.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433891929448 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140433891929448</title>\n",
       "<polygon fill=\"none\" points=\"460.5,-166.5 460.5,-212.5 831.5,-212.5 831.5,-166.5 460.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559\" y=\"-185.8\">decoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"657.5,-166.5 657.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"685\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"657.5,-189.5 712.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"685\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"712.5,-166.5 712.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"712.5,-189.5 831.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"772\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 140432530515896&#45;&gt;140433891929448 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140432530515896-&gt;140433891929448</title>\n",
       "<path d=\"M646,-249.366C646,-241.152 646,-231.658 646,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"649.5,-222.607 646,-212.607 642.5,-222.607 649.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432637700696 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140432637700696</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 442,-212.5 442,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-185.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-166.5 148,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-189.5 203,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-197.3\">(None, 17, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 442,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-174.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140432637701816&#45;&gt;140432637700696 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140432637701816-&gt;140432637700696</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432530516232 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140432530516232</title>\n",
       "<polygon fill=\"none\" points=\"194,-83.5 194,-129.5 672,-129.5 672,-83.5 194,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268\" y=\"-102.8\">decoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"342,-83.5 342,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"369.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"342,-106.5 397,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"369.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"397,-83.5 397,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"534.5\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"397,-106.5 672,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"534.5\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140433891929448&#45;&gt;140432530516232 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140433891929448-&gt;140432530516232</title>\n",
       "<path d=\"M588.078,-166.473C560.962,-156.162 528.498,-143.816 500.297,-133.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"501.501,-129.805 490.91,-129.522 499.013,-136.348 501.501,-129.805\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432637700696&#45;&gt;140432530516232 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140432637700696-&gt;140432530516232</title>\n",
       "<path d=\"M278.65,-166.473C305.639,-156.162 337.95,-143.816 366.019,-133.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"367.269,-136.361 375.362,-129.522 364.771,-129.822 367.269,-136.361\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433892090600 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140433892090600</title>\n",
       "<polygon fill=\"none\" points=\"275,-0.5 275,-46.5 591,-46.5 591,-0.5 275,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-19.8\">decoder_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"417,-0.5 417,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"417,-23.5 472,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"472,-0.5 472,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.5\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"472,-23.5 591,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.5\" y=\"-8.3\">(None, None, 310)</text>\n",
       "</g>\n",
       "<!-- 140432530516232&#45;&gt;140433892090600 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140432530516232-&gt;140433892090600</title>\n",
       "<path d=\"M433,-83.3664C433,-75.1516 433,-65.6579 433,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"436.5,-56.6068 433,-46.6068 429.5,-56.6069 436.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input_data = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_input_data = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_target_data will be ahead by one timestep\n",
    "# and will not include the start token.\n",
    "decoder_target_data = np.zeros(y_train_one_hot.shape)\n",
    "decoder_target_data[:,:-1] = y_train_one_hot[:,1:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_target_data_test = np.zeros(y_test_one_hot.shape)\n",
    "decoder_target_data_test[:,:-1] = y_test_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0 193 127  59  60 134  27 103  78  12  45 192 134  89  66 110] [171 301 260  10 134 142  93  38 207 132 189  99 305  38 224 304 122   0\n",
      "   0   0   0   0   0   0] [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[171 301] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(3499, 17) (3499, 24) (3499, 24, 310)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data[0], decoder_input_data[0], decoder_target_data[0])\n",
    "print(decoder_input_data[0][:2], decoder_target_data[0][:1])\n",
    "\n",
    "print(encoder_input_data.shape, decoder_input_data.shape, \n",
    "      decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "#model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', \n",
    "                                cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 1501 samples\n",
      "Epoch 1/1000\n",
      "3499/3499 [==============================] - 29s 8ms/step - loss: 4.1515 - val_loss: 3.3765\n",
      "Epoch 2/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 2.9331 - val_loss: 2.4794\n",
      "Epoch 3/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 2.1316 - val_loss: 1.7942\n",
      "Epoch 4/1000\n",
      "3499/3499 [==============================] - 26s 8ms/step - loss: 1.5697 - val_loss: 1.3812\n",
      "Epoch 5/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 1.2643 - val_loss: 1.1737\n",
      "Epoch 6/1000\n",
      "3499/3499 [==============================] - 26s 8ms/step - loss: 1.0943 - val_loss: 1.0367\n",
      "Epoch 7/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.9791 - val_loss: 0.9479\n",
      "Epoch 8/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.8999 - val_loss: 0.8830\n",
      "Epoch 9/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.8430 - val_loss: 0.8334\n",
      "Epoch 10/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.7925 - val_loss: 0.7915\n",
      "Epoch 11/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.7505 - val_loss: 0.7551\n",
      "Epoch 12/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.7130 - val_loss: 0.7226\n",
      "Epoch 13/1000\n",
      "3499/3499 [==============================] - 26s 8ms/step - loss: 0.6769 - val_loss: 0.6888\n",
      "Epoch 14/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.6439 - val_loss: 0.6594\n",
      "Epoch 15/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.6164 - val_loss: 0.6356\n",
      "Epoch 16/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.5855 - val_loss: 0.6078\n",
      "Epoch 17/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.5610 - val_loss: 0.5872\n",
      "Epoch 18/1000\n",
      "3499/3499 [==============================] - 26s 8ms/step - loss: 0.5369 - val_loss: 0.5683\n",
      "Epoch 19/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.5120 - val_loss: 0.5453\n",
      "Epoch 20/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.4877 - val_loss: 0.5180\n",
      "Epoch 21/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.4552 - val_loss: 0.4973\n",
      "Epoch 22/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.4258 - val_loss: 0.4621\n",
      "Epoch 23/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.3928 - val_loss: 0.4366\n",
      "Epoch 24/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.3610 - val_loss: 0.4025\n",
      "Epoch 25/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.3294 - val_loss: 0.3852\n",
      "Epoch 26/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.2966 - val_loss: 0.3497\n",
      "Epoch 27/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.2651 - val_loss: 0.3210\n",
      "Epoch 28/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.2359 - val_loss: 0.2946\n",
      "Epoch 29/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.2098 - val_loss: 0.2732\n",
      "Epoch 30/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.1849 - val_loss: 0.2508\n",
      "Epoch 31/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.1623 - val_loss: 0.2300\n",
      "Epoch 32/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.1435 - val_loss: 0.2114\n",
      "Epoch 33/1000\n",
      "3499/3499 [==============================] - 26s 8ms/step - loss: 0.1260 - val_loss: 0.2025\n",
      "Epoch 34/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.1129 - val_loss: 0.1857\n",
      "Epoch 35/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.0973 - val_loss: 0.1706\n",
      "Epoch 36/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.0861 - val_loss: 0.1658\n",
      "Epoch 37/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.0777 - val_loss: 0.1580\n",
      "Epoch 38/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.0697 - val_loss: 0.1481\n",
      "Epoch 39/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.0627 - val_loss: 0.1469\n",
      "Epoch 40/1000\n",
      "3499/3499 [==============================] - 27s 8ms/step - loss: 0.0586 - val_loss: 0.1406\n",
      "Epoch 41/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.0547 - val_loss: 0.1372\n",
      "Epoch 42/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.0466 - val_loss: 0.1308\n",
      "Epoch 43/1000\n",
      "3499/3499 [==============================] - 28s 8ms/step - loss: 0.0419 - val_loss: 0.1330\n",
      "Epoch 44/1000\n",
      "3499/3499 [==============================] - 29s 8ms/step - loss: 0.0390 - val_loss: 0.1279\n",
      "Epoch 45/1000\n",
      "3499/3499 [==============================] - 23s 7ms/step - loss: 0.0361 - val_loss: 0.1277\n",
      "Epoch 46/1000\n",
      "3499/3499 [==============================] - 28s 8ms/step - loss: 0.0339 - val_loss: 0.1258\n",
      "Epoch 47/1000\n",
      "3499/3499 [==============================] - 28s 8ms/step - loss: 0.0322 - val_loss: 0.1247\n",
      "Epoch 48/1000\n",
      "3499/3499 [==============================] - 26s 7ms/step - loss: 0.0285 - val_loss: 0.1199\n",
      "Epoch 49/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.0259 - val_loss: 0.1187\n",
      "Epoch 50/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0234 - val_loss: 0.1196\n",
      "Epoch 51/1000\n",
      "3499/3499 [==============================] - 20s 6ms/step - loss: 0.0225 - val_loss: 0.1187\n",
      "Epoch 52/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0201 - val_loss: 0.1178\n",
      "Epoch 53/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0188 - val_loss: 0.1178\n",
      "Epoch 54/1000\n",
      "3499/3499 [==============================] - 21s 6ms/step - loss: 0.0184 - val_loss: 0.1172\n",
      "Epoch 55/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.0180 - val_loss: 0.1211\n",
      "Epoch 56/1000\n",
      "3499/3499 [==============================] - 23s 7ms/step - loss: 0.0162 - val_loss: 0.1158\n",
      "Epoch 57/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0138 - val_loss: 0.1150\n",
      "Epoch 58/1000\n",
      "3499/3499 [==============================] - 21s 6ms/step - loss: 0.0124 - val_loss: 0.1154\n",
      "Epoch 59/1000\n",
      "3499/3499 [==============================] - 21s 6ms/step - loss: 0.0114 - val_loss: 0.1137\n",
      "Epoch 60/1000\n",
      "3499/3499 [==============================] - 23s 7ms/step - loss: 0.0104 - val_loss: 0.1147\n",
      "Epoch 61/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0102 - val_loss: 0.1158\n",
      "Epoch 62/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0095 - val_loss: 0.1168\n",
      "Epoch 63/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0111 - val_loss: 0.1190\n",
      "Epoch 64/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0344 - val_loss: 0.1617\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 65/1000\n",
      "3499/3499 [==============================] - 21s 6ms/step - loss: 0.0208 - val_loss: 0.1143\n",
      "Epoch 66/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0099 - val_loss: 0.1125\n",
      "Epoch 67/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0082 - val_loss: 0.1110\n",
      "Epoch 68/1000\n",
      "3499/3499 [==============================] - 22s 6ms/step - loss: 0.0073 - val_loss: 0.1115\n",
      "Epoch 69/1000\n",
      "3499/3499 [==============================] - 25s 7ms/step - loss: 0.0069 - val_loss: 0.1117\n",
      "Epoch 70/1000\n",
      "3499/3499 [==============================] - 20s 6ms/step - loss: 0.0065 - val_loss: 0.1112\n",
      "Epoch 71/1000\n",
      "3499/3499 [==============================] - 21s 6ms/step - loss: 0.0061 - val_loss: 0.1118\n",
      "Epoch 72/1000\n",
      "3499/3499 [==============================] - 20s 6ms/step - loss: 0.0059 - val_loss: 0.1113\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 73/1000\n",
      "3499/3499 [==============================] - 23s 7ms/step - loss: 0.0055 - val_loss: 0.1118\n",
      "Epoch 74/1000\n",
      "3499/3499 [==============================] - 23s 7ms/step - loss: 0.0054 - val_loss: 0.1120\n",
      "Epoch 75/1000\n",
      "3499/3499 [==============================] - 20s 6ms/step - loss: 0.0053 - val_loss: 0.1117\n",
      "Epoch 76/1000\n",
      "3499/3499 [==============================] - 24s 7ms/step - loss: 0.0052 - val_loss: 0.1121\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 77/1000\n",
      "3499/3499 [==============================] - 20s 6ms/step - loss: 0.0051 - val_loss: 0.1120\n",
      "Epoch 00077: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb96702bf98>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=([X_test, y_test], decoder_target_data_test),\n",
    "          callbacks=[lr_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:2368: UserWarning: Layer decoder_lstm_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We have trained a model, but how do we use it to actually translate sentences? We have to do more work ourselves here than with a non-recurrent neural net, so we'll write a function to help out. Here are the steps:\n",
    "\n",
    "1. **Encode**:\n",
    "    1. Run the entire input sentence through the encoder part of the model.\n",
    "    1. Write down the \"context vector\" -- this is the state of the last LSTM encoder layer.<br><br>\n",
    "\n",
    "2. **Decode in a loop**:\n",
    "    1. Seed the decoder LSTM with the context vector.\n",
    "    1. Run a *single step* of the decoder with the input \"`<S>`\" (the start symbol).\n",
    "    1. Store the output. This is a word of the translation!\n",
    "    1. Return to step 2B, but feed in the word from step 2C as the new input. Repeat until the decoder returns \"`</S>`\" (the end symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "encoder_masking (Masking)    (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Embedding (None, 17, 100)           19700     \n",
      "_________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)        [(None, 256), (None, 256) 365568    \n",
      "=================================================================\n",
      "Total params: 385,268\n",
      "Trainable params: 385,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states1)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 450.00 304.00\" width=\"450pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 446,-300 446,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140432637700976 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140432637700976</title>\n",
       "<polygon fill=\"none\" points=\"75,-249.5 75,-295.5 367,-295.5 367,-249.5 75,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-268.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-249.5 236,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-272.5 291,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-249.5 291,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-280.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"291,-272.5 367,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-257.3\">(None, 17)</text>\n",
       "</g>\n",
       "<!-- 140432637701592 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140432637701592</title>\n",
       "<polygon fill=\"none\" points=\"71.5,-166.5 71.5,-212.5 370.5,-212.5 370.5,-166.5 71.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-185.8\">encoder_masking: Masking</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-166.5 239.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-189.5 294.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-166.5 294.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.5\" y=\"-197.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-189.5 370.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.5\" y=\"-174.3\">(None, 17)</text>\n",
       "</g>\n",
       "<!-- 140432637700976&#45;&gt;140432637701592 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140432637700976-&gt;140432637701592</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432637701816 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140432637701816</title>\n",
       "<polygon fill=\"none\" points=\"43,-83.5 43,-129.5 399,-129.5 399,-83.5 43,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-102.8\">encoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"240,-83.5 240,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"240,-106.5 295,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"295,-83.5 295,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"347\" y=\"-114.3\">(None, 17)</text>\n",
       "<polyline fill=\"none\" points=\"295,-106.5 399,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"347\" y=\"-91.3\">(None, 17, 100)</text>\n",
       "</g>\n",
       "<!-- 140432637701592&#45;&gt;140432637701816 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140432637701592-&gt;140432637701816</title>\n",
       "<path d=\"M221,-166.366C221,-158.152 221,-148.658 221,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-139.607 221,-129.607 217.5,-139.607 224.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432637700696 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140432637700696</title>\n",
       "<polygon fill=\"none\" points=\"-2.84217e-14,-0.5 -2.84217e-14,-46.5 442,-46.5 442,-0.5 -2.84217e-14,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-19.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-0.5 148,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-23.5 203,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-0.5 203,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-31.3\">(None, 17, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-23.5 442,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-8.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140432637701816&#45;&gt;140432637700696 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140432637701816-&gt;140432637700696</title>\n",
       "<path d=\"M221,-83.3664C221,-75.1516 221,-65.6579 221,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-56.6068 221,-46.6068 217.5,-56.6069 224.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_masking (Masking)       (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 100)    31000       decoder_masking[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      decoder_embedding[0][0]          \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 310)    79670       decoder_lstm_1[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 476,238\n",
      "Trainable params: 476,238\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_state_input_h = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs1, state_h1, state_c1 = decoder_lstm_1(\n",
    "    decoder_inputs_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states1 = [state_h1, state_c1]\n",
    "decoder_outputs = decoder_dense(decoder_outputs1)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states1)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 941.00 387.00\" width=\"941pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 937,-383 937,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140432530516120 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140432530516120</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-332.5 31.5,-378.5 339.5,-378.5 339.5,-332.5 31.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"192.5,-332.5 192.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"192.5,-355.5 247.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"247.5,-332.5 247.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"247.5,-355.5 339.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.5\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140432530515896 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140432530515896</title>\n",
       "<polygon fill=\"none\" points=\"28,-249.5 28,-295.5 343,-295.5 343,-249.5 28,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112\" y=\"-268.8\">decoder_masking: Masking</text>\n",
       "<polyline fill=\"none\" points=\"196,-249.5 196,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"196,-272.5 251,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"251,-249.5 251,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"251,-272.5 343,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140432530516120&#45;&gt;140432530515896 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140432530516120-&gt;140432530515896</title>\n",
       "<path d=\"M185.5,-332.366C185.5,-324.152 185.5,-314.658 185.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189,-305.607 185.5,-295.607 182,-305.607 189,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433891929448 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140433891929448</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 371,-212.5 371,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-185.8\">decoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"197,-166.5 197,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"197,-189.5 252,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"252,-166.5 252,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311.5\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"252,-189.5 371,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311.5\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 140432530515896&#45;&gt;140433891929448 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140432530515896-&gt;140433891929448</title>\n",
       "<path d=\"M185.5,-249.366C185.5,-241.152 185.5,-231.658 185.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189,-222.607 185.5,-212.607 182,-222.607 189,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140432530516232 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140432530516232</title>\n",
       "<polygon fill=\"none\" points=\"281.5,-83.5 281.5,-129.5 759.5,-129.5 759.5,-83.5 281.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355.5\" y=\"-102.8\">decoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"429.5,-83.5 429.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"429.5,-106.5 484.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"484.5,-83.5 484.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"622\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"484.5,-106.5 759.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"622\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140433891929448&#45;&gt;140432530516232 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140433891929448-&gt;140432530516232</title>\n",
       "<path d=\"M276.598,-166.473C320.822,-155.78 374.089,-142.901 419.547,-131.91\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"420.523,-135.274 429.42,-129.522 418.878,-128.471 420.523,-135.274\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433627380256 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140433627380256</title>\n",
       "<polygon fill=\"none\" points=\"389,-166.5 389,-212.5 652,-212.5 652,-166.5 389,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"451.5\" y=\"-185.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"514,-166.5 514,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"541.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"514,-189.5 569,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"541.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"569,-166.5 569,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"569,-189.5 652,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140433627380256&#45;&gt;140432530516232 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140433627380256-&gt;140432530516232</title>\n",
       "<path d=\"M520.5,-166.366C520.5,-158.152 520.5,-148.658 520.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"524,-139.607 520.5,-129.607 517,-139.607 524,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433627380928 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140433627380928</title>\n",
       "<polygon fill=\"none\" points=\"670,-166.5 670,-212.5 933,-212.5 933,-166.5 670,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"732.5\" y=\"-185.8\">input_2: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"795,-166.5 795,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"822.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"795,-189.5 850,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"822.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"850,-166.5 850,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"850,-189.5 933,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"891.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140433627380928&#45;&gt;140432530516232 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140433627380928-&gt;140432530516232</title>\n",
       "<path d=\"M725.087,-166.473C688.536,-155.937 644.62,-143.279 606.863,-132.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"607.476,-128.929 596.898,-129.522 605.537,-135.655 607.476,-128.929\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140433892090600 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140433892090600</title>\n",
       "<polygon fill=\"none\" points=\"362.5,-0.5 362.5,-46.5 678.5,-46.5 678.5,-0.5 362.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433.5\" y=\"-19.8\">decoder_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"504.5,-0.5 504.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"532\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"504.5,-23.5 559.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"532\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"559.5,-0.5 559.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"619\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"559.5,-23.5 678.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"619\" y=\"-8.3\">(None, None, 310)</text>\n",
       "</g>\n",
       "<!-- 140432530516232&#45;&gt;140433892090600 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140432530516232-&gt;140433892090600</title>\n",
       "<path d=\"M520.5,-83.3664C520.5,-75.1516 520.5,-65.6579 520.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"524,-56.6068 520.5,-46.6068 517,-56.6069 524,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def translate_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    h1, c1 = encoder_model.predict(input_seq)\n",
    "    states_value1 = [h1, c1]\n",
    "    # Generate empty target sequence of length 1 (one-hot encoded).\n",
    "    #target_seq = np.zeros((1, num_output_words))\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first word of target sequence with the start symbol.\n",
    "    #target_seq[0, word_to_index2['<S>']] = 1.\n",
    "    target_seq[0,0] = word_to_index2['<S>']\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    step = 0\n",
    "    while not stop_condition:\n",
    "        #print('step:', step)\n",
    "        #print(states_value1[0][0][0:5])\n",
    "    \n",
    "        output_tokens, h1, c1  = decoder_model.predict(\n",
    "            [target_seq] + states_value1)\n",
    "\n",
    "        # Sample a token\n",
    "        #print(output_tokens)\n",
    "        #sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_word = index_to_word2[sampled_token_index]\n",
    "        #print(sampled_word)\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "        step += 1\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '</S>' or step > max_output_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, num_output_words))\n",
    "        #target_seq[0, sampled_token_index] = 1.\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update states\n",
    "        states_value1 = [h1, c1]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'most', 'loved', 'fruit', 'is', 'the', 'banana', ',', 'but', 'their', 'most', 'loved', 'is', 'the', 'orange', '.', '<MASK>']\n",
      "mon fruit le plus aimé est la banane , mais leur plus aimé est l'orange . </S> \n",
      "['she', 'likes', 'apples', ',', 'oranges', ',', 'and', 'strawberries', '.', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>']\n",
      "elle aime les pommes , les oranges et les fraises . </S> \n",
      "['the', 'banana', 'is', 'our', 'least', 'favorite', 'fruit', ',', 'but', 'the', 'grape', 'is', 'their', 'least', 'favorite', '.', '<MASK>']\n",
      "la banane est notre fruit préféré moins , mais le raisin est leur moins préféré . </S> \n",
      "['paris', 'is', 'usually', 'chilly', 'during', 'december', ',', 'and', 'it', 'is', 'never', 'cold', 'in', 'april', '.', '<MASK>', '<MASK>']\n",
      "paris est généralement froid en décembre , et il ne fait jamais froid en avril . </S> \n",
      "['he', 'likes', 'mangoes', ',', 'oranges', ',', 'and', 'lemons', '.', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>', '<MASK>']\n",
      "il aime les mangues , les oranges , les citrons et les . </S> \n",
      "['india', 'is', 'sometimes', 'rainy', 'during', 'january', ',', 'and', 'it', 'is', 'pleasant', 'in', 'february', '.', '<MASK>', '<MASK>', '<MASK>']\n",
      "l' inde est parfois pluvieux en janvier , et il est agréable en février . </S> \n",
      "['china', 'is', 'usually', 'relaxing', 'during', 'june', ',', 'but', 'it', 'is', 'never', 'quiet', 'in', 'summer', '.', '<MASK>', '<MASK>']\n",
      "chine est relaxant habituellement en juin , mais il est jamais calme en été . </S> \n",
      "['paris', 'is', 'freezing', 'during', 'fall', ',', 'and', 'it', 'is', 'never', 'dry', 'in', 'spring', '.', '<MASK>', '<MASK>', '<MASK>']\n",
      "paris est le gel à l'automne , et il est jamais à sec au printemps . </S> \n",
      "['new', 'jersey', 'is', 'sometimes', 'snowy', 'during', 'july', ',', 'but', 'it', 'is', 'sometimes', 'hot', 'in', 'december', '.', '<MASK>']\n",
      "new jersey est parfois enneigée en juillet , mais il est parfois chaud en décembre . </S> \n",
      "['china', 'is', 'sometimes', 'chilly', 'during', 'august', ',', 'but', 'it', 'is', 'usually', 'wonderful', 'in', 'january', '.', '<MASK>', '<MASK>']\n",
      "la chine est parfois froid au mois d' août , mais il est généralement merveilleux en janvier . </S> \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(indices_to_sentence(X_test[i], index_to_word1)[::-1])\n",
    "    print(translate_sequence(np.expand_dims(X_test[i], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
