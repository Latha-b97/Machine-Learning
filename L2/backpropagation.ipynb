{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Neural Net Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous assignment we used Keras to train a neural network. In this assignment you will build your own minimal neural net library. The basic structure is given to you; you will need to fill in details such as weight updating for backpropogation. Then you will test the network on learning the XOR function.\n",
    "\n",
    "Read through the class definitions below first to understand the basic architecture.\n",
    "\n",
    "Then you should add code as necessary where marked \"TODO\" in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNet():\n",
    "    \"\"\"Implements a basic feedforward neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._layers = []  # An ordered list of layers. The first layer is the input; the final is the output.\n",
    "    \n",
    "    def _add_layer(self, layer):\n",
    "        if self._layers:\n",
    "            # Update pointers. We keep a doubly-linked-list of layers for convenience.\n",
    "            prev_layer = self._layers[-1]\n",
    "            prev_layer.set_next_layer(layer)\n",
    "            layer.set_prev_layer(prev_layer)\n",
    "            \n",
    "        self._layers.append(layer)\n",
    "    \n",
    "    def add_input_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Input layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        layer = InputLayer(size=size, **kwargs)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def add_dense_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Dense layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        # Find the previous layer's size.\n",
    "        prev_size = self._layers[-1].size()\n",
    "        layer = DenseLayer(shape=(prev_size, size), **kwargs)\n",
    "        self._add_layer(layer)\n",
    "        \n",
    "    def train_single_example(self, X_data, y_data, learning_rate=0.01):\n",
    "        # Train on a single example. X_data and y_data must be numpy arrays.\n",
    "        \n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "\n",
    "        # Forward propagation.\n",
    "        outputs = self.predict(X_data)\n",
    "        \n",
    "        # Backpropagation.\n",
    "        deltas = outputs - y_data  # Compute error on this example.        \n",
    "        for layer in reversed(self._layers):\n",
    "            deltas = layer.backpropagate(deltas, learning_rate)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        \n",
    "        prev_output = x\n",
    "        for layer in self._layers:\n",
    "            prev_output = layer.feed_forward(prev_output)\n",
    "        return prev_output\n",
    "\n",
    "    def summary(self, verbose=False):\n",
    "        \"\"\"Prints a description of the model.\"\"\"\n",
    "        for i, layer in enumerate(self._layers):\n",
    "            print('%d: %s' % (i, str(layer)))\n",
    "            if verbose:\n",
    "                print('weights:', layer.get_weights())\n",
    "                if layer._use_bias:\n",
    "                    print('bias:', layer._bias)\n",
    "                print()\n",
    "    \n",
    "    def train(self, X_data, y_data, learning_rate, num_epochs, verbose=True, print_every_n=100):\n",
    "        \"\"\"Both X_data and y_data should be ndarrays. One example per row.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "\n",
    "        idx = list(range(len(X_data)))\n",
    "        for epoch in range(num_epochs):    \n",
    "            np.random.shuffle(idx)\n",
    "            for i in idx:\n",
    "                nnet.train_single_example(X_data[i], y_data[i], 0.1)\n",
    "            if verbose and (epoch % print_every_n == 0):\n",
    "                results = []\n",
    "                for i in range(len(X_data)):\n",
    "                    results.append(nnet.predict(X_data[i]))\n",
    "                mse = self.compute_mean_squared_error(X_data, y_data)\n",
    "                acc = self.compute_accuracy(X_data, y_data) * 100\n",
    "                print('%d: MSE: %.5f Acc: %.1f%% -- %s' % (epoch, mse, acc, results))\n",
    "    \n",
    "    def compute_mean_squared_error(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, compute and return the mean squared error.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        error = 0\n",
    "        for i in range(len(X_data)):\n",
    "            outputs = self.predict(X_data[i])\n",
    "            error += (y_data[i] - outputs) ** 2\n",
    "        mse = error / len(X_data)\n",
    "        return mse\n",
    "    \n",
    "    def compute_accuracy(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, convert outputs to binary using a threshold of 0.5\n",
    "        and return the accuracy: # examples correct / total # examples.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            outputs = self.predict(X_data[i])\n",
    "            outputs = outputs > 0.5\n",
    "            if outputs == y_data[i]:\n",
    "                correct += 1\n",
    "        acc = float(correct) / len(X_data)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    \"\"\"Class that represents an activation function and knows how to take its own derivative.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def activate(x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IdentityActivation(Activation):\n",
    "    \"\"\"Activation function that passes input through unchanged.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(name='Identity')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        return 1\n",
    "    \n",
    "class SigmoidActivation(Activation):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Sigmoid')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        return y * (1.0 - y)\n",
    "    \n",
    "class ReluActivation(Activation):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name='ReLU')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        if isinstance(x, np.ndarray):\n",
    "            result = np.zeros(x.shape)\n",
    "            for i in len(result):\n",
    "                result[i] = x[i] if x[i] > 0 else 0\n",
    "        else:\n",
    "            result = x if x > 0 else 0\n",
    "        return result\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        return y > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WeightInitializer():\n",
    "    \"\"\"Function to return a random weight from -1 to 1.\"\"\"\n",
    "    return np.random.random()*2 - 1\n",
    "\n",
    "def WeightInitializerPositive():\n",
    "    \"\"\"Function to return a random weight from 0 to 1.\"\"\"\n",
    "    return np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"Base class for NNet layers.\n",
    "    \n",
    "    Conceptually, in this library a Layer consists at a high level of:\n",
    "      * a collection of weights (a 2D numpy array)\n",
    "      * the output nodes that come after the weights above\n",
    "      * the activation function that is applied to the summed signals in these output nodes\n",
    "      \n",
    "    So a Layer isn't just nodes -- it's weights as well as nodes.\n",
    "      \n",
    "    Specifically, to send signal forward through a 3-layer network, we start with an Input Layer that does\n",
    "    very little.  The outputs from the Input layer are simply the fed-in input data.  \n",
    "    \n",
    "    Then, the next layer will be a Dense layer that holds the weights from the Input layer to the first hidden\n",
    "    layer and stores the activation function to be used after doing a product of weights and Input-Layer\n",
    "    outputs.\n",
    "    \n",
    "    Finally, another Dense layer will hold the weights from the hidden to the output layer nodes, and stores\n",
    "    the activation function to be applied to the final output nodes.\n",
    "    \n",
    "    For a typical 1-hidden layer network, then, we would have 1 Input layer and 2 Dense layers.\n",
    "    \n",
    "    Each Layer also has funcitons to perform the forward-pass and backpropagation steps for the weights/nodes\n",
    "    associated with the layer.\n",
    "    \n",
    "    Finally, each Layer stores pointers to the pervious and next layers, for convenience when implementing\n",
    "    backprop.\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, shape, use_bias, activation_function=IdentityActivation, weight_initializer=None, name=''):\n",
    "        # These are the weights from the *previous* layer to the current layer.\n",
    "        self._weights = None\n",
    "        \n",
    "        # Tuple of (# inputs, # outputs) for Dense layers or just a scalar for an input layer.\n",
    "        assert type(shape).__name__ == 'int' or type(shape).__name__ == 'tuple', (\n",
    "            'shape must be scalar or a 2-element tuple')\n",
    "        if type(shape).__name__ == 'tuple':\n",
    "            assert len(shape)==2, 'shape must be 2-dimensional. Was %d instead' % len(shape)\n",
    "        self._shape = shape \n",
    "    \n",
    "        # True to use a bias node that inputs to each node in this layer; False otherwise.\n",
    "        self._use_bias = use_bias\n",
    "        \n",
    "        if use_bias:\n",
    "            bias_size = shape[-1] if len(shape) > 1 else shape\n",
    "            self._bias = np.zeros(bias_size)\n",
    "            if weight_initializer:\n",
    "                for i in range(bias_size):\n",
    "                    self._bias[i] = weight_initializer()\n",
    "        \n",
    "        # Activation function to be applied to each dot product of weights with inputs.\n",
    "        # Instantiate an object of this class.\n",
    "        self._activation_function = activation_function() if activation_function else None\n",
    "        \n",
    "        # Method used to initialize the weights in this Layer at creation time.\n",
    "        self._weight_initializer = weight_initializer\n",
    "        \n",
    "        # Layer name (optional)\n",
    "        self._name = name\n",
    "        \n",
    "        # Calculated output vector from the most recent feed_forward(inputs) call.\n",
    "        self._outputs = None\n",
    "        \n",
    "        # Doubly linked list pointers to neighbor layers.\n",
    "        self._prev_layer = None  # Previous layer is closer to (or is) the input layer.\n",
    "        self._next_layer = None  # Next layer is closer to (or is) the output layer.\n",
    "    \n",
    "    def set_prev_layer(self, layer):\n",
    "        \"\"\"Set pointer to the previous layer.\"\"\"\n",
    "        self._prev_layer = layer\n",
    "    \n",
    "    def set_next_layer(self, layer):\n",
    "        \"\"\"Set pointer to the next layer.\"\"\"\n",
    "        self._next_layer = layer\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Number of nodes in this layer.\"\"\"\n",
    "        if type(self._shape).__name__ == 'tuple':\n",
    "            return self._shape[-1]\n",
    "        else:\n",
    "            return self._shape\n",
    "        \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return a numpy array of the weights for inputs to this layer.\"\"\"\n",
    "        return self._weights\n",
    "    \n",
    "    def get_bias(self):\n",
    "        \"\"\"Return a numpy array of the bias for nodes in this layer.\"\"\"\n",
    "        return self._biass\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
    "        \n",
    "        Also returns the outputs vector for convenience.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        raise NotimplementedError()\n",
    "        \n",
    "    def __str__(self):\n",
    "        activation_fxn_name = self._activation_function.name if self._activation_function else None\n",
    "        return '[%s] shape %s, use_bias=%s, activation=%s' % (self._name, self._shape, self._use_bias,\n",
    "                                                              activation_fxn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    \"\"\"A neural network 1-dimensional input layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, name='Input'):\n",
    "        assert type(size).__name__ == 'int', 'Input size must be integer. Was %s instead' % type(size).__name__\n",
    "        super().__init__(shape=size, use_bias=False, name=name, activation_function=None)\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs))\n",
    "        self._outputs = inputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        return None  # Nothing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"A neural network layer that is fully connected to the previous layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, shape, use_bias=True, name='Dense', **kwargs):\n",
    "        super().__init__(shape=shape, use_bias=use_bias, name=name, **kwargs)\n",
    "        \n",
    "        self._weights = np.zeros(shape)\n",
    "        if self._weight_initializer:\n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    self._weights[i,j] = self._weight_initializer()\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        self._outputs = np.dot(inputs, self._weights) + self._bias\n",
    "        for idx, val in np.ndenumerate(self._outputs):\n",
    "            self._outputs[idx] = self._activation_function.activate(val)\n",
    "        return self._outputs\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        assert isinstance(error, np.ndarray)\n",
    "        assert isinstance(self._prev_layer._outputs, np.ndarray)\n",
    "        assert isinstance(self._outputs, np.ndarray)  \n",
    "        \n",
    "        # Compute deltas. If this is the last layer, use the simpler backprop formula.\n",
    "        if self._next_layer is None:    \n",
    "            deltas = self._activation_function.derivative_given_y(self._outputs) * error\n",
    "        else:\n",
    "            deltas = np.matmul(self._next_layer._weights, error) * self._activation_function.derivative_given_y(self._outputs)\n",
    "            \n",
    "        gradient = np.matmul(np.expand_dims(self._prev_layer._outputs, 0).T, np.expand_dims(deltas, 0))\n",
    "                             \n",
    "        # Adjust weights.\n",
    "        self._weights -= learning_rate * gradient\n",
    "        \n",
    "        # Adjust bias weights.\n",
    "        if self._use_bias:\n",
    "            self._bias -= learning_rate * deltas\n",
    "        \n",
    "        return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y_data = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n"
     ]
    }
   ],
   "source": [
    "nnet = NNet()\n",
    "nnet.add_input_layer(2)\n",
    "nnet.add_dense_layer(2, weight_initializer=WeightInitializer, activation_function=SigmoidActivation)\n",
    "nnet.add_dense_layer(1, weight_initializer=WeightInitializer, activation_function=SigmoidActivation, name='Output')\n",
    "nnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "weights: None\n",
      "\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "weights: [[-0.76245984  0.64441248]\n",
      " [ 0.60957632 -0.50917349]]\n",
      "bias: [ 0.21541581 -0.58466165]\n",
      "\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
      "weights: [[-0.56239624]\n",
      " [ 0.52416319]]\n",
      "bias: [-0.92886289]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: MSE: 0.30270 Acc: 50.0% -- [array([0.26380321]), array([0.30173363]), array([0.23842]), array([0.27132486])]\n",
      "100: MSE: 0.24987 Acc: 75.0% -- [array([0.48421576]), array([0.52719147]), array([0.45623555]), array([0.49576903])]\n",
      "200: MSE: 0.24962 Acc: 50.0% -- [array([0.49291044]), array([0.53372361]), array([0.46483322]), array([0.50169645])]\n",
      "300: MSE: 0.24943 Acc: 50.0% -- [array([0.49468357]), array([0.53393251]), array([0.46619149]), array([0.50083671])]\n",
      "400: MSE: 0.24923 Acc: 75.0% -- [array([0.49480667]), array([0.5330573]), array([0.46568025]), array([0.49856992])]\n",
      "500: MSE: 0.24902 Acc: 75.0% -- [array([0.49700415]), array([0.53477791]), array([0.46706776]), array([0.49862123])]\n",
      "600: MSE: 0.24878 Acc: 75.0% -- [array([0.49837467]), array([0.53619836]), array([0.4674367]), array([0.49800979])]\n",
      "700: MSE: 0.24850 Acc: 75.0% -- [array([0.49761613]), array([0.53610514]), array([0.46542627]), array([0.49539508])]\n",
      "800: MSE: 0.24817 Acc: 75.0% -- [array([0.49814854]), array([0.53784282]), array([0.46454174]), array([0.49417379])]\n",
      "900: MSE: 0.24776 Acc: 75.0% -- [array([0.49962627]), array([0.5412029]), array([0.46440765]), array([0.49401273])]\n",
      "1000: MSE: 0.24725 Acc: 75.0% -- [array([0.49925073]), array([0.54346713]), array([0.46218432]), array([0.4920133])]\n",
      "1100: MSE: 0.24661 Acc: 75.0% -- [array([0.49921559]), array([0.54700451]), array([0.46008637]), array([0.49042987])]\n",
      "1200: MSE: 0.24581 Acc: 75.0% -- [array([0.49902226]), array([0.55144024]), array([0.45762044]), array([0.48869295])]\n",
      "1300: MSE: 0.24478 Acc: 75.0% -- [array([0.49836935]), array([0.55674773]), array([0.45447212]), array([0.48649404])]\n",
      "1400: MSE: 0.24347 Acc: 75.0% -- [array([0.49776239]), array([0.56369771]), array([0.4512079]), array([0.48435519])]\n",
      "1500: MSE: 0.24182 Acc: 75.0% -- [array([0.49630636]), array([0.57173036]), array([0.44698685]), array([0.48136751])]\n",
      "1600: MSE: 0.23974 Acc: 75.0% -- [array([0.4948536]), array([0.58203471]), array([0.44276455]), array([0.47838805])]\n",
      "1700: MSE: 0.23715 Acc: 75.0% -- [array([0.49189]), array([0.59349434]), array([0.43717352]), array([0.47394229])]\n",
      "1800: MSE: 0.23401 Acc: 75.0% -- [array([0.48742139]), array([0.60623079]), array([0.4304852]), array([0.46802443])]\n",
      "1900: MSE: 0.23029 Acc: 75.0% -- [array([0.48212257]), array([0.62100958]), array([0.42360105]), array([0.46133836])]\n",
      "2000: MSE: 0.22601 Acc: 75.0% -- [array([0.4759564]), array([0.63758277]), array([0.41675221]), array([0.45385799])]\n",
      "2100: MSE: 0.22126 Acc: 75.0% -- [array([0.46857585]), array([0.65482115]), array([0.40996105]), array([0.44519761])]\n",
      "2200: MSE: 0.21614 Acc: 75.0% -- [array([0.46099944]), array([0.67310139]), array([0.40435542]), array([0.43633848])]\n",
      "2300: MSE: 0.21074 Acc: 75.0% -- [array([0.45288847]), array([0.69083279]), array([0.39985584]), array([0.42672369])]\n",
      "2400: MSE: 0.20509 Acc: 75.0% -- [array([0.44561025]), array([0.70847596]), array([0.39795158]), array([0.41754428])]\n",
      "2500: MSE: 0.19914 Acc: 75.0% -- [array([0.43885541]), array([0.72450752]), array([0.39879909]), array([0.40818345])]\n",
      "2600: MSE: 0.19268 Acc: 75.0% -- [array([0.432983]), array([0.73860339]), array([0.40329716]), array([0.39855794])]\n",
      "2700: MSE: 0.18533 Acc: 75.0% -- [array([0.42774395]), array([0.75080561]), array([0.4121296]), array([0.38815296])]\n",
      "2800: MSE: 0.17657 Acc: 75.0% -- [array([0.42345053]), array([0.7611425]), array([0.42760574]), array([0.37721188])]\n",
      "2900: MSE: 0.16576 Acc: 75.0% -- [array([0.41786896]), array([0.76926153]), array([0.4499469]), array([0.36415738])]\n",
      "3000: MSE: 0.15233 Acc: 75.0% -- [array([0.4098687]), array([0.77635782]), array([0.48102611]), array([0.34925203])]\n",
      "3100: MSE: 0.13623 Acc: 100.0% -- [array([0.39619149]), array([0.78264369]), array([0.51929997]), array([0.33108431])]\n",
      "3200: MSE: 0.11826 Acc: 100.0% -- [array([0.37615884]), array([0.78928417]), array([0.56299243]), array([0.31009671])]\n",
      "3300: MSE: 0.10005 Acc: 100.0% -- [array([0.35069662]), array([0.7968584]), array([0.6082648]), array([0.28722235])]\n",
      "3400: MSE: 0.08330 Acc: 100.0% -- [array([0.32264242]), array([0.80549298]), array([0.65147709]), array([0.2641841])]\n",
      "3500: MSE: 0.06902 Acc: 100.0% -- [array([0.29495802]), array([0.81489457]), array([0.6900857]), array([0.24239887])]\n",
      "3600: MSE: 0.05742 Acc: 100.0% -- [array([0.26959844]), array([0.82465515]), array([0.72329355]), array([0.22292586])]\n",
      "3700: MSE: 0.04823 Acc: 100.0% -- [array([0.24706558]), array([0.8338835]), array([0.75099541]), array([0.2056517])]\n",
      "3800: MSE: 0.04100 Acc: 100.0% -- [array([0.22745311]), array([0.84250677]), array([0.7738061]), array([0.19049869])]\n",
      "3900: MSE: 0.03528 Acc: 100.0% -- [array([0.21076675]), array([0.85062314]), array([0.79297076]), array([0.17756258])]\n",
      "4000: MSE: 0.03072 Acc: 100.0% -- [array([0.196454]), array([0.85800176]), array([0.80907536]), array([0.16637588])]\n",
      "4100: MSE: 0.02705 Acc: 100.0% -- [array([0.18408451]), array([0.86470493]), array([0.82263668]), array([0.15662692])]\n",
      "4200: MSE: 0.02404 Acc: 100.0% -- [array([0.17333992]), array([0.87075942]), array([0.83419489]), array([0.14808305])]\n",
      "4300: MSE: 0.02156 Acc: 100.0% -- [array([0.16400171]), array([0.8762764]), array([0.84421596]), array([0.14060298])]\n",
      "4400: MSE: 0.01949 Acc: 100.0% -- [array([0.1557635]), array([0.88127105]), array([0.85289984]), array([0.13395802])]\n",
      "4500: MSE: 0.01773 Acc: 100.0% -- [array([0.14844931]), array([0.88580659]), array([0.8604936]), array([0.12802203])]\n",
      "4600: MSE: 0.01624 Acc: 100.0% -- [array([0.14196873]), array([0.88998977]), array([0.86724789]), array([0.12273846])]\n",
      "4700: MSE: 0.01495 Acc: 100.0% -- [array([0.13610289]), array([0.89376925]), array([0.87319855]), array([0.11792329])]\n",
      "4800: MSE: 0.01383 Acc: 100.0% -- [array([0.1308078]), array([0.89724466]), array([0.87851995]), array([0.1135567])]\n",
      "4900: MSE: 0.01285 Acc: 100.0% -- [array([0.12603373]), array([0.90046846]), array([0.88335196]), array([0.10960703])]\n",
      "5000: MSE: 0.01199 Acc: 100.0% -- [array([0.12168234]), array([0.90345185]), array([0.88772327]), array([0.10599222])]\n",
      "5100: MSE: 0.01123 Acc: 100.0% -- [array([0.11769341]), array([0.90621187]), array([0.89169442]), array([0.10266516])]\n",
      "5200: MSE: 0.01055 Acc: 100.0% -- [array([0.11402326]), array([0.90877356]), array([0.89532113]), array([0.0995937])]\n",
      "5300: MSE: 0.00994 Acc: 100.0% -- [array([0.11064684]), array([0.91117065]), array([0.89866269]), array([0.09676187])]\n",
      "5400: MSE: 0.00939 Acc: 100.0% -- [array([0.10751317]), array([0.91339995]), array([0.90173428]), array([0.09412383])]\n",
      "5500: MSE: 0.00890 Acc: 100.0% -- [array([0.10459622]), array([0.91548445]), array([0.90456485]), array([0.09166106])]\n",
      "5600: MSE: 0.00845 Acc: 100.0% -- [array([0.10189117]), array([0.91745188]), array([0.90720661]), array([0.08937351])]\n",
      "5700: MSE: 0.00804 Acc: 100.0% -- [array([0.09936542]), array([0.91930476]), array([0.90966655]), array([0.08723416])]\n",
      "5800: MSE: 0.00766 Acc: 100.0% -- [array([0.09700042]), array([0.92105058]), array([0.91196383]), array([0.08522693])]\n",
      "5900: MSE: 0.00732 Acc: 100.0% -- [array([0.09477371]), array([0.92269437]), array([0.91410375]), array([0.08333196])]\n",
      "6000: MSE: 0.00700 Acc: 100.0% -- [array([0.09267807]), array([0.92424923]), array([0.91610995]), array([0.08154529])]\n",
      "6100: MSE: 0.00671 Acc: 100.0% -- [array([0.09070425]), array([0.92572316]), array([0.91799978]), array([0.07985993])]\n",
      "6200: MSE: 0.00644 Acc: 100.0% -- [array([0.08884365]), array([0.92712731]), array([0.91978465]), array([0.07827004])]\n",
      "6300: MSE: 0.00619 Acc: 100.0% -- [array([0.08707835]), array([0.92845733]), array([0.92146476]), array([0.07675867])]\n",
      "6400: MSE: 0.00596 Acc: 100.0% -- [array([0.08541001]), array([0.92972827]), array([0.92306023]), array([0.07532891])]\n",
      "6500: MSE: 0.00574 Acc: 100.0% -- [array([0.08382285]), array([0.93093749]), array([0.92456811]), array([0.07396665])]\n",
      "6600: MSE: 0.00554 Acc: 100.0% -- [array([0.08231228]), array([0.93209049]), array([0.92599793]), array([0.07266829])]\n",
      "6700: MSE: 0.00535 Acc: 100.0% -- [array([0.08087549]), array([0.93319444]), array([0.9273592]), array([0.07143214])]\n",
      "6800: MSE: 0.00517 Acc: 100.0% -- [array([0.07950272]), array([0.9342479]), array([0.92865267]), array([0.07024952])]\n",
      "6900: MSE: 0.00500 Acc: 100.0% -- [array([0.07819253]), array([0.93525859]), array([0.92988592]), array([0.06911975])]\n",
      "7000: MSE: 0.00484 Acc: 100.0% -- [array([0.07693974]), array([0.93622833]), array([0.93106274]), array([0.06803859])]\n",
      "7100: MSE: 0.00469 Acc: 100.0% -- [array([0.07574074]), array([0.9371591]), array([0.93218819]), array([0.06700294])]\n",
      "7200: MSE: 0.00455 Acc: 100.0% -- [array([0.07459257]), array([0.93805486]), array([0.93326581]), array([0.06601054])]\n",
      "7300: MSE: 0.00442 Acc: 100.0% -- [array([0.07349089]), array([0.93891581]), array([0.93429831]), array([0.06505747])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400: MSE: 0.00429 Acc: 100.0% -- [array([0.07243183]), array([0.93974362]), array([0.93528716]), array([0.06414039])]\n",
      "7500: MSE: 0.00418 Acc: 100.0% -- [array([0.07141364]), array([0.94054125]), array([0.93623603]), array([0.06325798])]\n",
      "7600: MSE: 0.00406 Acc: 100.0% -- [array([0.07043441]), array([0.94131054]), array([0.93714866]), array([0.06240885])]\n",
      "7700: MSE: 0.00396 Acc: 100.0% -- [array([0.06949263]), array([0.94205412]), array([0.93802793]), array([0.06159178])]\n",
      "7800: MSE: 0.00385 Acc: 100.0% -- [array([0.06858421]), array([0.94277185]), array([0.93887323]), array([0.06080304])]\n",
      "7900: MSE: 0.00376 Acc: 100.0% -- [array([0.06770726]), array([0.94346452]), array([0.93968712]), array([0.06004103])]\n",
      "8000: MSE: 0.00366 Acc: 100.0% -- [array([0.06686012]), array([0.94413425]), array([0.94047098]), array([0.05930447])]\n",
      "8100: MSE: 0.00357 Acc: 100.0% -- [array([0.06604251]), array([0.94478308]), array([0.94122848]), array([0.05859331])]\n",
      "8200: MSE: 0.00349 Acc: 100.0% -- [array([0.0652518]), array([0.9454111]), array([0.94195969]), array([0.05790507])]\n",
      "8300: MSE: 0.00341 Acc: 100.0% -- [array([0.06448726]), array([0.94601998]), array([0.942667]), array([0.05723937])]\n",
      "8400: MSE: 0.00333 Acc: 100.0% -- [array([0.06374661]), array([0.94660998]), array([0.94335024]), array([0.05659404])]\n",
      "8500: MSE: 0.00326 Acc: 100.0% -- [array([0.06302945]), array([0.94718263]), array([0.94401184]), array([0.05596892])]\n",
      "8600: MSE: 0.00319 Acc: 100.0% -- [array([0.06233412]), array([0.94773826]), array([0.94465234]), array([0.05536255])]\n",
      "8700: MSE: 0.00312 Acc: 100.0% -- [array([0.06165992]), array([0.94827805]), array([0.94527311]), array([0.05477433])]\n",
      "8800: MSE: 0.00305 Acc: 100.0% -- [array([0.06100609]), array([0.94880271]), array([0.94587569]), array([0.05420367])]\n",
      "8900: MSE: 0.00299 Acc: 100.0% -- [array([0.06037144]), array([0.94931317]), array([0.94646025]), array([0.05364951])]\n",
      "9000: MSE: 0.00293 Acc: 100.0% -- [array([0.05975446]), array([0.94980927]), array([0.94702705]), array([0.05311052])]\n",
      "9100: MSE: 0.00287 Acc: 100.0% -- [array([0.05915435]), array([0.95029138]), array([0.9475772]), array([0.05258602])]\n",
      "9200: MSE: 0.00281 Acc: 100.0% -- [array([0.05857084]), array([0.95076083]), array([0.94811177]), array([0.05207584])]\n",
      "9300: MSE: 0.00276 Acc: 100.0% -- [array([0.0580037]), array([0.95121858]), array([0.94863211]), array([0.05157988])]\n",
      "9400: MSE: 0.00271 Acc: 100.0% -- [array([0.05745117]), array([0.95166411]), array([0.94913756]), array([0.05109644])]\n",
      "9500: MSE: 0.00266 Acc: 100.0% -- [array([0.05691319]), array([0.9520985]), array([0.94962941]), array([0.05062556])]\n",
      "9600: MSE: 0.00261 Acc: 100.0% -- [array([0.05638947]), array([0.95252236]), array([0.95010875]), array([0.05016705])]\n",
      "9700: MSE: 0.00256 Acc: 100.0% -- [array([0.05587871]), array([0.95293558]), array([0.95057512]), array([0.04971971])]\n",
      "9800: MSE: 0.00252 Acc: 100.0% -- [array([0.05538139]), array([0.95333946]), array([0.95103034]), array([0.04928406])]\n",
      "9900: MSE: 0.00247 Acc: 100.0% -- [array([0.05489598]), array([0.9537335]), array([0.95147361]), array([0.0488587])]\n"
     ]
    }
   ],
   "source": [
    "nnet.train(X_data, y_data, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "weights: None\n",
      "\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "weights: [[-5.03889463  5.49296836]\n",
      " [ 4.76858006 -5.5461631 ]]\n",
      "bias: [-2.67446457 -3.15866281]\n",
      "\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
      "weights: [[7.43102137]\n",
      " [7.30890295]]\n",
      "bias: [-3.63205322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
