{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole Balancing using DQNs\n",
    "In this assignment we will balance a cartpole using deep learning. We will build an agent, that given the current state of the environment, can make a prediction about what action would result in the best outcome. We are going to implement the two core pieces of DQNs, the epsilon greedy algorithm and memory replay. \n",
    "\n",
    "In this assignment we will use openai gym libraries to set up the game enviroment. Most of the game playing interface is already provided by the gym library. Our task is to implement the agent, and fix up the training. As we play the game, you should see the agent's score increase in the training loop. A score of 100 or above is what we are trying to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only teacher can use pip3\r\n"
     ]
    }
   ],
   "source": [
    "# If you are running this practice on your machine, make sure to install gym and gym[atari]. Depending on your python \n",
    "# env, this could be done using pip install, or conda install etc. \n",
    "!pip3 install --user gym gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        ############################# TODO #####################################\n",
    "        # Create a \"simple\" network, with 2-4 layers. Your network \n",
    "        # takes in the observations so the input_dim should match \n",
    "        # the size of observation space. Your network should output the probability \n",
    "        # of taking each action, so it's output size should match the \n",
    "        # action_size. Keep the last layer's activation as linear and use \n",
    "        # mean squared error for loss. Return your model after compiling it. \n",
    "        ########################### END TODO ###################################\n",
    "        model.add(Dense(4, input_shape = (4,)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        ############################# TODO #####################################\n",
    "        # Create a tuple of state, action, reward, next_state and done \n",
    "        # and append this tuple to the memory.\n",
    "        ########################### END TODO ###################################\n",
    "        self.memory.append((state, action, reward,next_state, done))    \n",
    "                      \n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        # In this function we calculate and return the next action.\n",
    "        # We are going to implement epsilon greedy logic here. \n",
    "        # With probability epsilon, return a random action and return that\n",
    "        # With probability 1-epsilon return the action that the model predicts. \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "            \n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # We'll sample from our memories and get a handful of them and store them in minibatch \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma*self.model.predict(next_state).max()\n",
    "                # Calculate the total discounted reward according to the Q-Learning formula\n",
    "                # your formula should look something like this\n",
    "                # target = current_reward + discounted maximum value obtained by next state\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        # Decay the epsilon value \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0/40, score: 10, eps: 1.0\n",
      "episode: 1/40, score: 27, eps: 1.0\n",
      "episode: 2/40, score: 32, eps: 1.0\n",
      "episode: 3/40, score: 25, eps: 1.0\n",
      "episode: 4/40, score: 24, eps: 1.0\n",
      "episode: 5/40, score: 21, eps: 0.92\n",
      "episode: 6/40, score: 10, eps: 0.88\n",
      "episode: 7/40, score: 26, eps: 0.77\n",
      "episode: 8/40, score: 25, eps: 0.68\n",
      "episode: 9/40, score: 18, eps: 0.62\n",
      "episode: 10/40, score: 41, eps: 0.51\n",
      "episode: 11/40, score: 12, eps: 0.48\n",
      "episode: 12/40, score: 12, eps: 0.45\n",
      "episode: 13/40, score: 13, eps: 0.42\n",
      "episode: 14/40, score: 12, eps: 0.4\n",
      "episode: 15/40, score: 13, eps: 0.37\n",
      "episode: 16/40, score: 8, eps: 0.36\n",
      "episode: 17/40, score: 13, eps: 0.33\n",
      "episode: 18/40, score: 9, eps: 0.32\n",
      "episode: 19/40, score: 11, eps: 0.3\n",
      "episode: 20/40, score: 35, eps: 0.25\n",
      "episode: 21/40, score: 57, eps: 0.19\n",
      "episode: 22/40, score: 41, eps: 0.15\n",
      "episode: 23/40, score: 64, eps: 0.11\n",
      "episode: 24/40, score: 81, eps: 0.075\n",
      "episode: 25/40, score: 125, eps: 0.04\n",
      "episode: 26/40, score: 166, eps: 0.017\n",
      "episode: 27/40, score: 105, eps: 0.01\n",
      "episode: 28/40, score: 117, eps: 0.01\n",
      "episode: 29/40, score: 82, eps: 0.01\n",
      "episode: 30/40, score: 149, eps: 0.01\n",
      "episode: 31/40, score: 137, eps: 0.01\n",
      "episode: 32/40, score: 254, eps: 0.01\n",
      "episode: 33/40, score: 99, eps: 0.01\n",
      "episode: 34/40, score: 137, eps: 0.01\n",
      "episode: 35/40, score: 97, eps: 0.01\n",
      "episode: 36/40, score: 90, eps: 0.01\n",
      "episode: 37/40, score: 168, eps: 0.01\n",
      "episode: 38/40, score: 60, eps: 0.01\n",
      "episode: 39/40, score: 54, eps: 0.01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # TODO What's the state size for CartPole game?\n",
    "    state_size = 4\n",
    "    # TODO What's the action size for CartPole game?\n",
    "    action_size = 2\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 128 # Feel free to play with these \n",
    "    EPISODES = 40   # You shouldn't really need more than 100 episodes to get a score of 100\n",
    "\n",
    "    \n",
    "    for eps in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            \n",
    "            # TODO Get an action from the agent\n",
    "            action = agent.act(state)\n",
    "            # TODO Send this action to the env and get the next_state, reward, done values\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # DO NOT CHANGE THE FOLLOWING 2 LINES \n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # TODO Tell the agent to remember this memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # DO NOT CHANGE BELOW THIS LINE\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, eps: {:.2}\".format(eps, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        if eps % 10 == 0:\n",
    "            agent.save(\"./cartpole-dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
