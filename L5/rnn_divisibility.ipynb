{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 5b: Recurrent Neural Networks: Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Recurrent Neural Networks (RNNs) can be used in many different ways, such as classification, single-step prediction, and the generation of an entire sequence. \n",
    "\n",
    "* **Classification**: the input is a sequence, and the output is a single category - this is the focus of this assignment. (Alternatively, a sequence of categories could be generated, one for each partial sequence as it is processed).\n",
    "\n",
    "* **Prediction**: the input is a sequence, and the output is a prediction for the next element in the sequence. You will explore this in lesson 5b.\n",
    "\n",
    "* **Sequence Generation** (Seq-to-Seq): both the input and the output are entire sequences. For example, RNN-based language translation may take in an input sequence (of characters or word tokens) in English, and generate as output a sequence (of characters or word tokens) in French.\n",
    "\n",
    "RNNs can be used to process inputs that occur naturally in time (such as an audio recording of speech or music represented as a stream of timestamped MIDI messages), but they can also be applied to material that has an order to it, even if it's not necessarly temporal in natures, such as written text (which can be read one character or one word at a time) or even written numbers or math equations (which can be read one digit or symbol at a time, from left to right, for instance.)  This is the problem we investigate today: looking at numbers such as \"1423\" as a sequence of digits ['1', '4', '2', '3'].\n",
    "\n",
    "Our problem comes curtosy of Distinguished Professor Douglas R. Hofstadter of Indiana University, author of books such as _Gödel, Escher, Bach: an Eternal Golden Braid_. Hofstadter writes [private communication, shared with permission)]:\n",
    "\n",
    "---\n",
    "\n",
    "_Lately, I have been musing about the seeming power of deep neural nets.  They learn to recognize members of all sorts of categories, when those members (and non-members) are fed to them as patterns of symbols or of pixels.  So, how about the following challenges involving the natural numbers?_\n",
    "\n",
    "* To recognize the even numbers, expressed in base 3.\n",
    "     (Specifically, 0, 2, 11, 20, 22, 101, 110, 112,...)\n",
    "* To recognize the multiples of 3, expressed in base 10.\n",
    "* To recognize the multiples of 9, expressed in base 10.\n",
    "* To recognize the multiples of 7, expressed in base 10.\n",
    "* To recognize the multiples of 29, expressed in base 10.\n",
    "\n",
    "_(I suppose that if a net can learn any particular one of the above list, it can learn all of them.  Just a guess...)_\n",
    "\n",
    " _Moving right along, how about the following somewhat harder challenges?_\n",
    "\n",
    "* To recognize the correct integer additions, expressed either in base 2 or in base 10.  (For example, the string “12+29=41”.)\n",
    "* To recognize the correct integer multiplications, expressed either in base 2 or in base 10.  (For example, the string “12x29=348”.)\n",
    "\n",
    "_(The latter of this pair seems significantly harder than the former.)_\n",
    "     \n",
    "_And then, of course, the canonical challenge of this sort:_\n",
    "\n",
    "* To recognize the prime numbers, expressed either in base 2 or in base 10.\n",
    "\n",
    "_Each of the above challenges involves a number-theoretical category that can easily be described in purely syntactic terms (i.e., as a rule-based pattern of symbols).  It would be trivial to generate millions of examples of such categories mechanically, and then you just feed them to the neural net.  You can also feed the network lots of counterexamples -- marking them, of course, as non-members of the category.  Can a deep neural network learn any of these categories?  All of them?  Some of them?_\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you will use an RNN to try to solve the divisibility-by-3 problem (the rest are challenges you might want to try in your free time!): \n",
    "\n",
    "* **\"To recognize the multiples of 3, expressed in base 10.\"**  Specifically, you must:\n",
    "    * Design an RNN that takes a sequence of digits as input. \n",
    "    * Represent digits in base 10 by using a categorical, one-hot encoding, with one node for each digit from 0 through 9.\n",
    "    * Train the RNN to categorize a number as True if it is evenly divisible by 3, False otherwise.\n",
    "    * Test the RNN on a set of previously-unseen numbers, including numbers that are 4 digits long, such as 2225 and 3333.\n",
    "    * Acheive an accuracy of at least 95% on the test set (report the accuracy in the cell marked below).\n",
    "    * Answer the questions at the end of this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout,Activation, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up params for dataset.\n",
    "DIVISIBILITY_NUMBER = 3         # We want to test for divisilibity by 3.\n",
    "TRAIN_TEST_SPLIT = 0.7          # Percentage of data in training set\n",
    "NUM_EXAMPLES_PER_CLASS = 1000      # Generate the first 1000 multiples of 3 for training/testing\n",
    "                                # Also generate 1000 non-multiples of 3.\n",
    "NUM_CATEGORIES = 10             # 10 digits\n",
    "MAX_DIGITS = 5                  # Number of digits allowed in input strings\n",
    "\n",
    "# Neural net hyperparameters-- just an example. Adjust these as needed.\n",
    "BATCH_SIZE = 32\n",
    "NUM_LSTM_NODES = 512  # 10       \n",
    "DROPOUT = 0.5 \n",
    "LEARING_RATE = 0.001\n",
    "NUM_EPOCHS = 500 #500 #50 ####\n",
    "\n",
    "# TODO: add/modify constants as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Helper functions to generate the dataset of training/testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_example_numbers(base_number=DIVISIBILITY_NUMBER, num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Return a tuple of two lists: (list_of_multiples, list_of_nonmultiples).\n",
    "    \n",
    "    For example, ([0, 3, 6, 9, 12, ...2997], [1, 4, 5, 8, 11, 13, 14,...,2999]).\n",
    "    Each list contains num_examples_per_class elements.\n",
    "    \"\"\"\n",
    "    multiples =[]\n",
    "    non_multiples = []\n",
    "    \n",
    "    for x in range(num_examples_per_class):\n",
    "        multiples.append(random.randint(1,33333) * base_number)\n",
    "        non_multiples.append((random.randint(1,33333) * base_number) + 1)\n",
    "    return (multiples, non_multiples)\n",
    "\n",
    "#generatedNumbers = generate_example_numbers()\n",
    "#for x in range(50):\n",
    "#    print(generatedNumbers[0][x])\n",
    "#    print(generatedNumbers[1][x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(size_multiples = NUM_EXAMPLES_PER_CLASS, size_non_multiples= NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Return two list of labels one for the True case (multiples) and one for the False case (nonmultiples).\n",
    "    \n",
    "    Represent True as 1, False as 0.\n",
    "    For example, return ([1, 1, 1, 1.....], [0, 0, 0, 0, ....]) with each list the requested size.\n",
    "    \"\"\"\n",
    "    multiples_Label =[]\n",
    "    non_multiples_Label = []\n",
    "    \n",
    "    for x in range(size_multiples):\n",
    "        multiples_Label.append(1)\n",
    "        non_multiples_Label.append(0)\n",
    "    return (multiples_Label, non_multiples_Label)\n",
    "\n",
    "#generatedLabels = generate_labels()\n",
    "#for x in range(50):\n",
    "#    print(generatedLabels[0][x])\n",
    "#    print(generatedLabels[1][x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digit_to_vector(digit):\n",
    "    \"\"\"Given a digit from 0-9, return a numpy array representing the digit using a 1-hot encoding.\n",
    "    keras.utils.to_categorical may be useful.\n",
    "    \"\"\"\n",
    "    encoded = to_categorical(digit) \n",
    "    return encoded\n",
    "\n",
    "#encoded = digit_to_vector(4)\n",
    "#print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_to_input_example(number, max_digits=MAX_DIGITS):\n",
    "    \"\"\"Given an integer number, return a numpy float array of 0.0s and 1.0s, of the correct shape to feed into the \n",
    "    neural net.\n",
    "    \n",
    "    For example, if you have a max of 5 digits then you should have a 2D numpy matrix: 5 rows (one for each\n",
    "    sequence index), and 10 columns (1 for each digit).\n",
    "    \n",
    "    In order to train in \"batch\" mode, the RNN expects every example to have the same shape. So if you have a 2-digit\n",
    "    number such as \"42\", you need to pad the example with a \"padding\" token somehow; for example, \"???42\", and then\n",
    "    use keras.layers.Masking to ignore the leading digits. Or just pad with 0s, as in \"00042\". \n",
    "    keras.preprocessing.sequence.pad_sequences can help with this.\n",
    "    \"\"\"\n",
    "    # convert given number to individual digit [123] to [1,2,3]\n",
    "    intarray = [int(d) for d in str(number)]\n",
    "    # make list of list\n",
    "    numlist = list(intarray)\n",
    "    seqlist = []\n",
    "    seqlist.append(numlist)\n",
    "    digit = pad_sequences(seqlist, maxlen=max_digits)\n",
    "    #print (digit)\n",
    "    return digit\n",
    "#number_to_input_example(123456)\n",
    "\n",
    "def generate_dataset(divisibility_number=DIVISIBILITY_NUMBER, train_test_split=TRAIN_TEST_SPLIT, \n",
    "                     num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Generate a dataset ready for training. Returns a list of tuples. Each tuple is of the form\n",
    "    (input_array, label). The dataset should be shuffled either here or during the training process to\n",
    "    mix divisile-by-DIVISIBILITY_NUMBER and not-divisible-by-DIVISIBILITY_NUMBER examples.\n",
    "    The dataset should consist of NUM_EXAMPLES_PER_CLASS positive examples (e.g., 1000 examples of divisible-by-3), and\n",
    "    also NUM_EXAMPLES_PER_CLASS negative examples (e.g., 1000 examples of not-divisible-by-3).\n",
    "    \"\"\"\n",
    "    \n",
    "    generatedDataSet = []\n",
    "    generatedNumbers = generate_example_numbers()\n",
    "    generatedLabels = generate_labels()\n",
    "    \n",
    "    for x in range(NUM_EXAMPLES_PER_CLASS):\n",
    "        generatedDataSet.append(tuple((generatedNumbers[0][x], generatedLabels[0][x])))\n",
    "        generatedDataSet.append(tuple((generatedNumbers[1][x], generatedLabels[1][x])))\n",
    "        \n",
    "    shuffle(generatedDataSet)\n",
    "    #for x in range(50):\n",
    "    #    print(generatedDataSet[x])\n",
    "    #    print(generatedDataSet[x][0])\n",
    "    #    print(generatedDataSet[x][1])\n",
    "        \n",
    "    return generatedDataSet\n",
    "        \n",
    "#generate_dataset()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Helper functions to generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build RNN model.\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #model.add(Embedding(NUM_CATEGORIES, output_dim=256))\n",
    "    model.add(Embedding(NUM_CATEGORIES, output_dim=512))\n",
    "\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=0.3, return_sequences=False, input_shape=(MAX_DIGITS, NUM_CATEGORIES)))  # Use return_sequences=True for multiple hidden layers\n",
    "    model.add(Dropout(0.5)) \n",
    "    #model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    \n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Model should return 1 or 0 for divisible/not-divisible\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 512)         5120      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,139,265\n",
      "Trainable params: 2,138,241\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model configuration.\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Generate dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =[]\n",
    "data = generate_dataset()\n",
    "X = [d[0] for d in data]\n",
    "y = [d[1] for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: build train/validate datasets.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X, y, test_size=0.33)\n",
    "#train_data = ...\n",
    "#validation_data = ...\n",
    "#train_inputs = ...\n",
    "#train_labels = ...\n",
    "#validation_inputs = ...\n",
    "#validation_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = Adam(lr=LEARING_RATE)   # Modify learning algorithm as needed\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1340 samples, validate on 660 samples\n",
      "Epoch 1/500\n",
      "1340/1340 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4955 - val_loss: 0.6934 - val_acc: 0.4606\n",
      "Epoch 2/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6930 - acc: 0.5194 - val_loss: 0.6937 - val_acc: 0.4606\n",
      "Epoch 3/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6929 - acc: 0.5194 - val_loss: 0.6941 - val_acc: 0.4606\n",
      "Epoch 4/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6928 - acc: 0.5194 - val_loss: 0.6945 - val_acc: 0.4606\n",
      "Epoch 5/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6927 - acc: 0.5194 - val_loss: 0.6948 - val_acc: 0.4606\n",
      "Epoch 6/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6926 - acc: 0.5194 - val_loss: 0.6950 - val_acc: 0.4606\n",
      "Epoch 7/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6926 - acc: 0.5194 - val_loss: 0.6952 - val_acc: 0.4606\n",
      "Epoch 8/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6954 - val_acc: 0.4606\n",
      "Epoch 9/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6956 - val_acc: 0.4606\n",
      "Epoch 10/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6957 - val_acc: 0.4606\n",
      "Epoch 11/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6958 - val_acc: 0.4606\n",
      "Epoch 12/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6959 - val_acc: 0.4606\n",
      "Epoch 13/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6961 - val_acc: 0.4606\n",
      "Epoch 14/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6961 - val_acc: 0.4606\n",
      "Epoch 15/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6962 - val_acc: 0.4606\n",
      "Epoch 16/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6964 - val_acc: 0.4606\n",
      "Epoch 17/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6964 - val_acc: 0.4606\n",
      "Epoch 18/500\n",
      "1340/1340 [==============================] - 1s 617us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6966 - val_acc: 0.4606\n",
      "Epoch 19/500\n",
      "1340/1340 [==============================] - 1s 646us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6965 - val_acc: 0.4606\n",
      "Epoch 20/500\n",
      "1340/1340 [==============================] - 1s 620us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6965 - val_acc: 0.4606\n",
      "Epoch 21/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6966 - val_acc: 0.4606\n",
      "Epoch 22/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 23/500\n",
      "1340/1340 [==============================] - 1s 624us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 24/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6966 - val_acc: 0.4606\n",
      "Epoch 25/500\n",
      "1340/1340 [==============================] - 1s 619us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 26/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 27/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 28/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 29/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 30/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 31/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 32/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 33/500\n",
      "1340/1340 [==============================] - 1s 651us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 34/500\n",
      "1340/1340 [==============================] - 1s 702us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 35/500\n",
      "1340/1340 [==============================] - 1s 636us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 36/500\n",
      "1340/1340 [==============================] - 1s 637us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 37/500\n",
      "1340/1340 [==============================] - 1s 634us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 38/500\n",
      "1340/1340 [==============================] - 1s 695us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 39/500\n",
      "1340/1340 [==============================] - 1s 674us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 40/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 41/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 42/500\n",
      "1340/1340 [==============================] - 1s 626us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 43/500\n",
      "1340/1340 [==============================] - 1s 656us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 44/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 45/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 46/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 47/500\n",
      "1340/1340 [==============================] - 1s 670us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 48/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 49/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 50/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 51/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 52/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 53/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 54/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 55/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 56/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 57/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 58/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 59/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 61/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 62/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 63/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 64/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 65/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 66/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 67/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 68/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 69/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 70/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 71/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 72/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 73/500\n",
      "1340/1340 [==============================] - 1s 704us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 74/500\n",
      "1340/1340 [==============================] - 1s 655us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 75/500\n",
      "1340/1340 [==============================] - 1s 648us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 76/500\n",
      "1340/1340 [==============================] - 1s 661us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 77/500\n",
      "1340/1340 [==============================] - 1s 655us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 78/500\n",
      "1340/1340 [==============================] - 1s 625us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 79/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 80/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 81/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 82/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 83/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 84/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 85/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 86/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 87/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 88/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 89/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 90/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 91/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 92/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 93/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 94/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 95/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 96/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 97/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 98/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 99/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 100/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 101/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 102/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 103/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 104/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 105/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 106/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 107/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 108/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 109/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 110/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 111/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 112/500\n",
      "1340/1340 [==============================] - 1s 651us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 113/500\n",
      "1340/1340 [==============================] - 1s 653us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 114/500\n",
      "1340/1340 [==============================] - 1s 652us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 115/500\n",
      "1340/1340 [==============================] - 1s 681us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 116/500\n",
      "1340/1340 [==============================] - 1s 656us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 117/500\n",
      "1340/1340 [==============================] - 1s 634us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 118/500\n",
      "1340/1340 [==============================] - 1s 649us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340/1340 [==============================] - 1s 668us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 120/500\n",
      "1340/1340 [==============================] - 1s 677us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 121/500\n",
      "1340/1340 [==============================] - 1s 693us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 122/500\n",
      "1340/1340 [==============================] - 1s 626us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 123/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 124/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 125/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 126/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 127/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 128/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 129/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 130/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 131/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 132/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 133/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 134/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 135/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 136/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 137/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 138/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 139/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 140/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 141/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 142/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 143/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 144/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 145/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 146/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 147/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 148/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 149/500\n",
      "1340/1340 [==============================] - 1s 619us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 150/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 151/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 152/500\n",
      "1340/1340 [==============================] - 1s 704us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 153/500\n",
      "1340/1340 [==============================] - 1s 687us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 154/500\n",
      "1340/1340 [==============================] - 1s 687us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 155/500\n",
      "1340/1340 [==============================] - 1s 682us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 156/500\n",
      "1340/1340 [==============================] - 1s 674us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 157/500\n",
      "1340/1340 [==============================] - 1s 652us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 158/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 159/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 160/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 161/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 162/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 163/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 164/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 165/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 166/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 167/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 168/500\n",
      "1340/1340 [==============================] - 1s 617us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 169/500\n",
      "1340/1340 [==============================] - 1s 617us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 170/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 171/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 172/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 173/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 174/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 175/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 176/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 177/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 179/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 180/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 181/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 182/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 183/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 184/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 185/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 186/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 187/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 188/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 189/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 190/500\n",
      "1340/1340 [==============================] - 1s 623us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 191/500\n",
      "1340/1340 [==============================] - 1s 687us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 192/500\n",
      "1340/1340 [==============================] - 1s 670us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 193/500\n",
      "1340/1340 [==============================] - 1s 642us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 194/500\n",
      "1340/1340 [==============================] - 1s 660us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 195/500\n",
      "1340/1340 [==============================] - 1s 634us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 196/500\n",
      "1340/1340 [==============================] - 1s 693us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 197/500\n",
      "1340/1340 [==============================] - 1s 689us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 198/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 199/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 200/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 201/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 202/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 203/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 204/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 205/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 206/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 207/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 208/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 209/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 210/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 211/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 212/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 213/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 214/500\n",
      "1340/1340 [==============================] - 1s 617us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 215/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 216/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 217/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 218/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 219/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 220/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 221/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 222/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 223/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 224/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 225/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 226/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 227/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 228/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 229/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 230/500\n",
      "1340/1340 [==============================] - 1s 681us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 231/500\n",
      "1340/1340 [==============================] - 1s 673us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 232/500\n",
      "1340/1340 [==============================] - 1s 669us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 233/500\n",
      "1340/1340 [==============================] - 1s 700us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 234/500\n",
      "1340/1340 [==============================] - 1s 655us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 235/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 236/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 238/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 239/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 240/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 241/500\n",
      "1340/1340 [==============================] - 1s 643us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 242/500\n",
      "1340/1340 [==============================] - 1s 624us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 243/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 244/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 245/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 246/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 247/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 248/500\n",
      "1340/1340 [==============================] - 1s 639us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 249/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 250/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 251/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 252/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 253/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 254/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 255/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 256/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 257/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 258/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 259/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 260/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 261/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 262/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 263/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 264/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 265/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 266/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 267/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 268/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 269/500\n",
      "1340/1340 [==============================] - 1s 648us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 270/500\n",
      "1340/1340 [==============================] - 1s 700us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 271/500\n",
      "1340/1340 [==============================] - 1s 626us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 272/500\n",
      "1340/1340 [==============================] - 1s 635us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 273/500\n",
      "1340/1340 [==============================] - 1s 672us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 274/500\n",
      "1340/1340 [==============================] - 1s 658us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 275/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 276/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 277/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 278/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 279/500\n",
      "1340/1340 [==============================] - 1s 590us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 280/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 281/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 282/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 283/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 284/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 285/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 286/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 287/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 288/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 289/500\n",
      "1340/1340 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.516 - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 290/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 291/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 292/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 293/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 294/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 295/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 297/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 298/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 299/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 300/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 301/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 302/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 303/500\n",
      "1340/1340 [==============================] - 1s 591us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 304/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 305/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 306/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 307/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 308/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 309/500\n",
      "1340/1340 [==============================] - 1s 672us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 310/500\n",
      "1340/1340 [==============================] - 1s 643us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 311/500\n",
      "1340/1340 [==============================] - 1s 675us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 312/500\n",
      "1340/1340 [==============================] - 1s 652us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 313/500\n",
      "1340/1340 [==============================] - 1s 673us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 314/500\n",
      "1340/1340 [==============================] - 1s 650us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 315/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 316/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 317/500\n",
      "1340/1340 [==============================] - 1s 626us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 318/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 319/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 320/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 321/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 322/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 323/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 324/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 325/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 326/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 327/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 328/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 329/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 330/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 331/500\n",
      "1340/1340 [==============================] - 1s 620us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 332/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 333/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 334/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 335/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 336/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 337/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 338/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 339/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 340/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 341/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 342/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 343/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 344/500\n",
      "1340/1340 [==============================] - 1s 624us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 345/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 346/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 347/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 348/500\n",
      "1340/1340 [==============================] - 1s 679us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 349/500\n",
      "1340/1340 [==============================] - 1s 668us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 350/500\n",
      "1340/1340 [==============================] - 1s 670us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6972 - val_acc: 0.4606\n",
      "Epoch 351/500\n",
      "1340/1340 [==============================] - 1s 675us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 352/500\n",
      "1340/1340 [==============================] - 1s 670us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 353/500\n",
      "1340/1340 [==============================] - 1s 665us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 354/500\n",
      "1340/1340 [==============================] - 1s 617us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 356/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 357/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 358/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 359/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 360/500\n",
      "1340/1340 [==============================] - 1s 603us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 361/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 362/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 363/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 364/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 365/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 366/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 367/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 368/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 369/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 370/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 371/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 372/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 373/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 374/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 375/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 376/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 377/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 378/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 379/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 380/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 381/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 382/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 383/500\n",
      "1340/1340 [==============================] - 1s 662us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 384/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 385/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 386/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 387/500\n",
      "1340/1340 [==============================] - 1s 679us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 388/500\n",
      "1340/1340 [==============================] - 1s 663us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 389/500\n",
      "1340/1340 [==============================] - 1s 664us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 390/500\n",
      "1340/1340 [==============================] - 1s 683us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 391/500\n",
      "1340/1340 [==============================] - 1s 684us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 392/500\n",
      "1340/1340 [==============================] - 1s 630us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 393/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 394/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 395/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 396/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 397/500\n",
      "1340/1340 [==============================] - 1s 604us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 398/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 399/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 400/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 401/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 402/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 403/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 404/500\n",
      "1340/1340 [==============================] - 1s 616us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 405/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 406/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 407/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 408/500\n",
      "1340/1340 [==============================] - 1s 614us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 409/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 410/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 411/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 412/500\n",
      "1340/1340 [==============================] - 1s 613us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 413/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 415/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 416/500\n",
      "1340/1340 [==============================] - 1s 615us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 417/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 418/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 419/500\n",
      "1340/1340 [==============================] - 1s 610us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 420/500\n",
      "1340/1340 [==============================] - 1s 609us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 421/500\n",
      "1340/1340 [==============================] - 1s 607us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 422/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 423/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 424/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 425/500\n",
      "1340/1340 [==============================] - 1s 622us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 426/500\n",
      "1340/1340 [==============================] - 1s 689us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 427/500\n",
      "1340/1340 [==============================] - 1s 644us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 428/500\n",
      "1340/1340 [==============================] - 1s 631us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 429/500\n",
      "1340/1340 [==============================] - 1s 658us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 430/500\n",
      "1340/1340 [==============================] - 1s 673us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 431/500\n",
      "1340/1340 [==============================] - 1s 671us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 432/500\n",
      "1340/1340 [==============================] - 1s 640us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 433/500\n",
      "1340/1340 [==============================] - 1s 611us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 434/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 435/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 436/500\n",
      "1340/1340 [==============================] - 1s 612us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 437/500\n",
      "1340/1340 [==============================] - 1s 660us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 438/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 439/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 440/500\n",
      "1340/1340 [==============================] - 1s 600us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 441/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 442/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 443/500\n",
      "1340/1340 [==============================] - 1s 635us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 444/500\n",
      "1340/1340 [==============================] - 1s 637us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 445/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 446/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 447/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 448/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 449/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 450/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 451/500\n",
      "1340/1340 [==============================] - 1s 605us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 452/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 453/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 454/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 455/500\n",
      "1340/1340 [==============================] - 1s 602us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6967 - val_acc: 0.4606\n",
      "Epoch 456/500\n",
      "1340/1340 [==============================] - 1s 606us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 457/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 458/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 459/500\n",
      "1340/1340 [==============================] - 1s 608us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 460/500\n",
      "1340/1340 [==============================] - 1s 601us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 461/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 462/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 463/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 464/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 465/500\n",
      "1340/1340 [==============================] - 1s 670us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 466/500\n",
      "1340/1340 [==============================] - 1s 693us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 467/500\n",
      "1340/1340 [==============================] - 1s 677us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 468/500\n",
      "1340/1340 [==============================] - 1s 648us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 469/500\n",
      "1340/1340 [==============================] - 1s 676us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 470/500\n",
      "1340/1340 [==============================] - 1s 650us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 471/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 472/500\n",
      "1340/1340 [==============================] - ETA: 0s - loss: 0.6924 - acc: 0.519 - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 474/500\n",
      "1340/1340 [==============================] - 1s 598us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 475/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 476/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 477/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 478/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 479/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 480/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 481/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 482/500\n",
      "1340/1340 [==============================] - 1s 590us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 483/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 484/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 485/500\n",
      "1340/1340 [==============================] - ETA: 0s - loss: 0.6921 - acc: 0.524 - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 486/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 487/500\n",
      "1340/1340 [==============================] - 1s 596us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6969 - val_acc: 0.4606\n",
      "Epoch 488/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 489/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 490/500\n",
      "1340/1340 [==============================] - 1s 597us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 491/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 492/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6925 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 493/500\n",
      "1340/1340 [==============================] - 1s 593us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 494/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n",
      "Epoch 495/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 496/500\n",
      "1340/1340 [==============================] - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 497/500\n",
      "1340/1340 [==============================] - 1s 592us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 498/500\n",
      "1340/1340 [==============================] - 1s 599us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6971 - val_acc: 0.4606\n",
      "Epoch 499/500\n",
      "1340/1340 [==============================] - 1s 595us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6970 - val_acc: 0.4606\n",
      "Epoch 500/500\n",
      "1340/1340 [==============================] - ETA: 0s - loss: 0.6926 - acc: 0.517 - 1s 594us/step - loss: 0.6924 - acc: 0.5194 - val_loss: 0.6968 - val_acc: 0.4606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc31e42f60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Configure for viewing validation loss/accuracy using the \"validation_data\" parameter.\n",
    "#model.fit(train_inputs, train_labels, validation_data=(validation_inputs, validation_labels), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, shuffle=True, verbose=1)\n",
    "\n",
    "model.fit(train_inputs, train_labels, validation_data=(validation_inputs, validation_labels), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, shuffle=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Report your final accuracy on the validation dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 104us/step\n",
      "loss: 69.68%\n",
      "accuracy: 46.06%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = model.evaluate(validation_inputs, validation_labels, verbose=1)\n",
    "print(\"loss: %.2f%%\" % (scores[0]*100))\n",
    "print(\"accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#TODO: accuracy = ??%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Examine model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32),\n",
       " array([[0.4810968]], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the outputs of the model on some test data.\n",
    "[model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot some results\n",
    "results = []\n",
    "lo = 0\n",
    "hi = 1000\n",
    "rng = range(lo,hi)\n",
    "for num in rng:\n",
    "    # Hint: to run on a single example, you can use \"np.expand_dims\" to add an extra \n",
    "    # dimension to a 2D array, in order to make a \"batch\" of 1.\n",
    "    #\n",
    "    # TODO something like this:\n",
    "    #print(num)\n",
    "    results.append(model.predict(number_to_input_example(num))[0][0])\n",
    "    \n",
    "#testnum = 234567\n",
    "#model.predict(number_to_input_example(testnum))[0][0]\n",
    "#[model.predict(np.expand_dims(number_to_input_example(testnum))[0][0], 0) for i in range(10)]\n",
    "#print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc31235a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rng[:100], results[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1) What happens if you give a 5-digit number or a 6-digit number to the trained model, after training on 1-, 2-, 3-, and 4- digit numbers?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For 5-digit, it works fine as it has trained for 5-digits. For 6-digits, the results would be random, as it was not trained before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "2) Pick another number from Hofstadter's list above, such as 9, 7, or 29. Train a model, and report the accuracy of your results. Did it work or not? Why or why not (your best guess)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I should work. It should accurately predict for those values. Even though traing has happened for randomlu generated numbers, my gues is that it leans that the number 3 is a facor of 9, but not 29 or 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "3) Record any other comments/insights from your model training process. What worked well? What caused trouble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My accuracy was always low. I tried by increasing number of epochs to 250, I tried increasing NUM_LSTM_NODES to 512, but still getting same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "4) If you didn't have a training algorithm, how would you design a RNN-style system to recognize divisibility by 3?\n",
    "Ignoring the details of the weights, what kind of state must be carried over from step to step as each digit is read in\n",
    "successively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Divisibility rule for number 3 is that sum of the individual digits should be divisible by 3. We should carry the sum of the digits as the state to be carried over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "5) BONUS (hard): Explain how the neural net you trained above works, with evidence from examining the node activations as the net runs. Does it do anything similar to what you would have designed as a human?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
